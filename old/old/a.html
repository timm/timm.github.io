<table>
<tr><td>2020 </td><td> JITTERBUG </td><td> deep learning  </td><td> self-admitted technical debt</td><td>[pdf](https://arxiv.org/pdf/2002.11049) Don't let the easy hide the hard</td></tr>
<tr><td>2020 </td><td> DECART </td><td> deep learning  </td><td> static code warnings </td><td>[pdf](https://arxiv.org/pdf/2006.00444) Sometimes, deep learning in SE is overkill</td></tr>
<tr><td>2020 </td><td> DECART </td><td> hyperparameter optimization</td><td> project health     </td><td>[pdf](https://arxiv.org/pdf/2007.02893) Asimov was right: humans (developers) are predictable </td></tr>
<tr><td>2020 </td><td>  </td><td> optimization</td><td> fairness     </td><td>[pdf](https://arxiv.org/pdf/2007.02893) Fairness is a choice and not choosing is not ethical </td></tr>
<tr><td>2020 </td><td> TERMINATOR </td><td> analytics</td><td> test case prioritization     </td><td>[pdf](https://arxiv.org/pdf/2008.00612) We don't need much to guess the future</td></tr>
<tr><td>2020 </td><td> GHOST </td><td> deep learning</td><td> defects     </td><td>[pdf](https://arxiv.org/pdf/2008.03835) Off-the shelf deep learning fails for SE</td></tr>
<tr><td>2020 </td><td> DODGE </td><td>hyperparameter optimization</td><td> many     </td><td>[pdf](https://arxiv.org/pdf/2006.07416) "Explanation", used properly, becomes a planning tool</td></tr>
<tr><td>2020 </td><td> TimeLIME </td><td>planning</td><td>defects     </td><td>[pdf](https://arxiv.org/pdf/2006.07416) "Explanation", used properly, becomes a planning tool</td></tr>
<tr><td>2019 </td><td> Termiantor </td><td>active learning</td><td>testing     </td><td>[pdf](https://arxiv.org/pdf/1905.07019) Humans and AI can learn together to which tests will fail sooner.</td></tr>
<tr><td>2018</td><td>             </td><td>software 4.0   </td><td>            </td><td>[pdf](https://arxiv.org/pdf/1809.00039) AI can learn what matters most to to a human operator.</td></tr>
<tr><td>2018</td><td> Fast2       </td><td>active learning</td><td>lit reviews </td><td>[pdf](https://arxiv.org/pdf/1705.05420.pdf)  Humans can train and AI what documents to they need to read.</td></tr>
<tr><td>2019 </td><td> Xtree      </td><td>planning       </td><td> defects    </td><td>[pdf](https://arxiv.org/pdf/1708.05442) We can learn plans on what to change to most improve quality.</td></tr>
<tr><td>2018</td><td>  Arima      </td><td> analytics</td><td> bugs, enhancements  </td><td>[pdf](https://arxiv.org/pdf/1710.08736.pdf) By looking at many projects, we can find very simple predictors of next month's bugs.</td></tr>
<tr><td>2018</td><td> XTREE       </td><td> planning      </td><td> bad smells </td><td>[pdf](https://arxiv.org/pdf/1609.03614.pdf) Intelligent defect predictors can stop developers wasting time while they are fixing bad smells.</td></tr>
<tr><td>2015</td><td>LACE2        </td><td> privacy       </td><td> defects    </td><td>[pdf](A simple feature and instance selectors let software projects share privatized data, without missing important patterns.)</td></tr>
<tr><td>2012</td><td> LSR         </td><td> analytics </td><td> defects    </td><td>[pdf](http://menzies.us/pdf/12better.pdf) Very simple social metrics can generate near-optimal predictors for software quality.</td></tr>
<tr><td>2012</td><td> WHICH2      </td><td>  analytics</td><td>defects     </td><td>[pdf[(http://menzies.us/pdf/10which.pdf) Static code defect predictors have inherent limitations. But these limits can fixed via a new learner, very simple learner, that better understand the business goals.</td></tr>
<tr><td>2007</td><td> Wrapper+DT  </td><td> analytics</td><td>defects     </td><td>[pdf](http://menzies.us/pdf/06learnPredict.pdf) Surprisingly effective defect predictors can be built from simple static code attributes.</td></tr>
<tr><td>2019</td><td> DODGE       </td><td> analytics    </td><td> security   </td><td>[pdf](https://arxiv.org/pdf/1911.02476.pdf) Security bugs can be uncovered even when hidden deep inside software data.</td></tr>
<tr><td>2018</td><td> DE+SVM      </td><td> text-mining  </td><td>Stackoverflow </td><td>[pdf](https://arxiv.org/pdf/1802.05319.pdf) Search-based methods can be 500 times faster than deep learners.</td></tr>
<tr><td>2017</td><td> DE+SVM      </td><td> text-mining   </td><td>Stackoverflow</td><td>[pdf](https://arxiv.org/pdf/1703.00133.pdf) Very simple optimizers can out perform overly complex deep learners.</td></tr>
<tr><td>2016</td><td> DE+RF       </td><td> quality       </td><td> defects     </td><td>[pdf](https://arxiv.org/pdf/1609.01759.pdf) Very simple optimizers can dramatically improve the performance of data miners learning software quality predictors.</td></tr>
<tr><td>2013</td><td> random projection</td><td>optimization</td><td> product lines </td><td>[pdf](http://menzies.us/pdf/15gale.pdf) Active learners can simplify and reduce the cost of search-based se by orders of magnitude.</td></tr>
<tr><td>2002</td><td>  TAR2       </td><td> planning      </td><td>   process   </td><td>[pdf](http://menzies.us/pdf/02truisms.pdf) Search-based SE methods can easily and readily and critically assess long held SE truisms.</td></tr>
<tr><td>2019</td><td> GENERAL     </td><td> transfer learning</td><td>defects   </td><td>[pdf](https://arxiv.org/pdf/1911.04250.pdf) We can find general lessons from 100s of software projects.</td></tr>
<tr><td>2018</td><td> XTREE       </td><td> planning         </td><td>  defects, code smells, effort           </td><td>[pdf](https://arxiv.org/pdf/1703.06218.pdf) The bellwether effect teaches us much about generality in SE.</td></tr>
<tr><td>2017</td><td>  KS         </td><td>transfer learning </td><td> defects  </td><td>[pdf](ihttp://menzies.us/pdf/17hdp.pdf) Even when project data collects data using different labels, we can still transfer lessons learned between them.</td></tr>
<tr><td>2016</td><td> XTREE       </td><td>transfer learning </td><td> defects</td><td>[pdf](http://fuwei.us/pdf/krishnaASE16.pdf) Ultra-simple transfer learning methods (called "bellwethers") enable effective transfer of lessons learned.</td></tr>
<tr><td>2013</td><td>  GAC        </td><td>transfer learning</td><td> effort </td><td>[pdf](http://menzies.us/pdf/13transferEffort.pdf) How to transfer lessons learned from past projects? Easy! Clustering tools enable transferring lessons learned between software projects.</td></tr>
<tr><td>2009</td><td>KNN+NB       </td><td>transfer learning</td><td> defects  </td><td>[pdf](http://menzies.us/pdf/08ccwc.pdf) A simple nearest neighbor relevancy filtering resulted in one of the first general results in software analytics: defect predictor learned from Turkish toasters could be successfully applied to NASA flight software (and vice versa).</td></tr>
<tr><td>2017</td><td>SHORT        </td><td>abduction        </td><td>requirements engineering</td><td>[pdf](https://arxiv.org/pdf/1702.05568.pdf) Seemingly complex, conflicting models can be tamed and controlled via some simple stochastic probing.</td></tr>
<tr><td>2003</td><td> TAR3        </td><td>contrast set learning</td><td>requirements engineering</td><td>[pdf](http://menzies.us/pdf/03tar2.pdf) Contrast set learners can explain enormous decision trees (6000 node) learned from complex requirements models just 6 rules</td></tr>
<tr><td>2002</td><td> TAR2        </td><td>contrast set learning</td><td>requirements engineering</td><td>[pdf](http://menzies.us/pdf/02re02.pdf) Contrast set learners find simple controllers in requirements models.</td></tr>
<tr><td>2016</td><td>  COCONUT    </td><td>  analytics         </td><td> effort, cocomo     </td><td>[pdf](https://arxiv.org/pdf/1609.05563.pdf) The effort to build complex software can be estimated by very simple equations.</td></tr>
<tr><td>2013</td><td>  Ensemble  </td><td>  active learning   </td><td>    effort          </td><td>[pdf](http://menzies.us/pdf/13active.pdf) Active learners can easily estimate large software projects after just a few samples.</td></tr>
<tr><td>2018</td><td> RIOT       </td><td> configuration </td><td> cloud </td><td>[pdf](https://arxiv.org/pdf/1708.08127.pdf) To tame cloud computing, use next generation algorithms</td></tr>
<tr><td>2016</td><td> LDADE      </td><td> topic modeling        </td><td>    bibliometrics  </td><td>[pdf](https://arxiv.org/pdf/1608.08100v2.pdf) Text miners can succinctly summarize thousands of technical papers about SE.</td></tr>
<tr><td>2015</td><td>             </td><td>feature engineering</td><td>  food management </td><td> [pdf](https://www.ncbi.nlm.nih.gov/pubmed/26208427) Data miners can greatly simplify and reduce the effort involved in data collection for community health studies.</td></tr>
<tr><td>2010</td><td> TAR3       </td><td>contrast set learning </td><td> spacecraft control </td><td>[pdf](https://ntrs.nasa.gov/archive/nasa/casi.ntrs.nasa.gov/20110010887.pdf) Simple contrast-set learners out-perform state-of-the-art optimizers for spacecraft control;</td></tr>
<tr><td>1990</td><td> PIGE       </td><td>   expert systems       </td><td>  farm management </td><td>[pdf](http://menzies.us/pdf/ukapril92.pdf) The lesson of decades of expert systems research is that, for specific domains, human expertise can be readily captured in just a few rules.</td></tr></td></tr>
</table>
