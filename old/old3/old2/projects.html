<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
    "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta name="generator" content=
  "HTML Tidy for Linux (vers 25 March 2009), see www.w3.org" />
  <meta http-equiv="content-type" content=
  "text/html; charset=iso-8859-1" />
  <meta name=
  "Tim Menzies is a full professor in Computer 
   Science at NcState exploring SE, data mining, AI and optimization."
   content="" />
<link href='http://fonts.googleapis.com/css?family=Marck+Script' rel='stylesheet' type='text/css'>
  <link href="img/default.css" rel="stylesheet" type="text/css" />
  <link rel="icon" href="img/favicon.png" type="image/png" />

  <link rel="stylesheet" href="http://maxcdn.bootstrapcdn.com/font-awesome/4.3.0/css/font-awesome.min.css">


  <title>Current research</title>   
</head>
  
<body>
<div id="wrapper"> 
<img src="img/r-header.png" style="width:380px; margin-top:-15px;
float:right; padding-bottom:10px;">
 
<div id="navcontainer">
<small><ul id="navlist">
<li><a  href="http://menzies.us" target="blank">Home</a></li>

<li> <a  href="http://scholar.google.com/citations?hl=en&user=7htTUTgmLtUC&view_op=list_works&sortby=pubdate" target="blank">Papers</a></li>

<li> <a  href="pdf/cv.pdf" target="blank">CV</a>  </li>
<li>   <a  href="projects.html" target="blank">Projects</a>  </li>
<li> <a  href="http://ai4se.net" target="blank">Vist my lab ...</a>  </li>
</ul></small>
</div>
    <div id="header">
      <table align=center width=760px>
        <tr>
          <td>
            <a href="index.html"><h1><b>Tim Menzies</b></a></h1></td>
<td align=right><a  
href="https://www.facebook.com/timmenzies"><img  id=f  src="img/icons/facebook-24.png"></a><a  
href="https://github.com/timm"><img  id=gi src="img/icons/github-24.png"></a><a  
href="https://www.dropbox.com/sh/2hkf4q271jg5t0o/AAAp9ggE1iMC5bgORnseJay7a?dl=0"><img id=d  src="img/icons/dropbox-24.png"></a><a  
href="https://twitter.com/timmenzies"><img id=t  src="img/icons/twitter-24.png"></a><a  
href="https://www.linkedin.com/profile/view?id=420532&trk=nav_responsive_tab_profile_pic"><img id=li src="img/icons/linkedin-24.png"></a><a 
href="http://slideshare.com/timmenzies"><img id=sl src="img/icons/slideshare-24.png"></a><a 
href="https://www.amazon.com/gp/registry/wishlist/ref=cm_wl_search_1?ie=UTF8&cid=A1Q2QWKIRXQU6O/"><img id=a   src="img/icons/amazon-24.png"></a><a
href="http://stackoverflow.com/users/1505323/tim-senzies"><img id=st src="img/icons/stackoverflow-24.png"></a><a 
href="https://www.goodreads.com/user/show/1469588-tim"><img id=go src="img/icons/goodreads-24.png"></a><a  
href="http://www.last.fm/user/timmenzies"><img   id=la src="img/icons/lastfm-24.png"></a><a
href="http://www.pinterest.com/timmenzies/"><img id=p  src="img/icons/pinterest-24.png"></a><a  
href="https://www.youtube.com/channel/UCVUryYA_9yxF2Y4Kln1cbVQ"><img  id=y  src="img/icons/youtube-24.png"></a><a
													  href="https://www.quora.com/Tim-Menzies-2"><img  id=q  src="img/icons/quora-24.png"></a><a  
href="https://medium.com/@timmenzies"><img   id=me style="padding:0px; margin: 0px; padding-bottom:7px; height:14px; width: 18px;"src="img/icons/medium.png"></a>


</td>
</tr>
<tr>
<td colspan=2 style="
		     text-align:center;  color: white;
padding-top: 15px;
		     margin-bottom:0px; padding-botton: 0px;
		     font-size: xx-small">tim.menzies@gmail.com
 :: 304-376-2859 (ph) ::
 919-515-7896 (fax) ::
 Computer Science, NC State University,
890 Oval Dr, Raleigh, NC, 27695-8206
</td>
</tr>
      </table>
    </div>
    
    
     
   
    <div id="content">
<div id=worklist>


    <table id=work align=center><tr>
<td>Prospective students: 
</td><td id=cone><a href="career.html#why-study-computer-science">Why CS ?</a> <font size="+1">&#x279c;</font> </td><td 
id=ctwo><a href="career.html#why-study-software-engineering">Why SE ?</a>  <font size="+1">&#x279c;</font> </td><td 
														   id=cthree><a href="career.html#why-study-se-artificial-intelligence">Why SE + AI ?</a>  <font size="+1">&#x279c;</font> </td>

<td 
   id=cfive><a href="projects.html">See my projects</a>  <font size="+1">&#x279c;</font> </td>

<td 
   id=cfour><a href="http://ai4se.net/people">Meet my students</a>  <font size="+1">&#x279c;</font> </td>


<td 
id=csix><a href="http://www.ncsu.edu/grad/programs/how-to-apply/index.php">Apply now!</a></td></tr></table>

</div>

<hr style="margin-bottom:15px; margin-top:15px;">


 <h1> Current research</h1>

                  <div id="TOC">&nbsp;<br>
        <ul>
        <li><a href="#industrial-ai-does-it-really-work">Industrial AI: Does it Really Work?</a></li>
        <li><a href="#tuning-compilers">Tuning Compilers</a></li>
        <li><a href="#tuning-data-miners">Tuning Data Miners</a></li>
        <li><a href="#truisms-in-se-true-or-false">Truisms in SE: True or False?</a></li>
        <li><a href="#transfer-learning-in-software-engineering">Transfer Learning in Software Engineering</a></li>
        <li><a href="#gale-geometric-active-learning-for-search-based-software-engineering">GALE: Geometric Active Learning for Search-Based Software Engineering</a></li>
        <li><a href="#evolutionary-search-with-strong-heuristics-for-software-product-line-configuration">Evolutionary Search with Strong Heuristics for Software Product Line Configuration</a></li>
        <li><a href="#lace2-better-privacy-preserving-data-sharing-for-cross-project-defect-prediction">LACE2: Better Privacy-Preserving Data Sharing for Cross Project Defect Prediction</a></li>
        <li><a href="#years-of-parametric-effort-estimation-a-report-card-on-cocomo-style-research">40 Years of Parametric Effort Estimation: A Report Card on COCOMO-style Research</a></li>
        <li><a href="#cross-trees-visualizing-estimations-using-decision-trees">Cross Trees: Visualizing Estimations using Decision Trees</a></li>
        </ul>
      </div>       <hr />
<h2 id="industrial-ai-does-it-really-work">Industrial AI: Does it Really Work?</h2>
<p><em>2015 -- </em></p>
<center>
<iframe src="http://www.slideshare.net/slideshow/embed_code/key/f8etbZ448ukfOs" width="425" height="355" frameborder="0" marginwidth="0" marginheight="0" scrolling="no" style="border:1px solid #CCC; border-width:1px;
margin-bottom:5px; max-width: 100%;" allowfullscreen>
</iframe>
<div style="margin-bottom:5px">
<strong> <a
href="http://www.slideshare.net/timmenzies/lexisnexis-june9"
title="Lexisnexis june9" target="_blank">Lexisnexis june9</a> </strong> from <strong><a
href="http://www.slideshare.net/timmenzies"
target="_blank">CS, NcState</a></strong>
</div>
</center>
<hr />
<h2 id="tuning-compilers">Tuning Compilers</h2>
<p>Increasingly, our society and research is making more use of computational methods. For example, in 2013 a Nobel Prize was awarded to chemists using computer models to explore very fast chemical reactions during photosynthesis. In the press release of the award, the Nobel Prize committee wrote <em>Today the computer is just as important a tool for chemists as the test tube</em>.</p>
<p>As with general science, so too with data analytics in software engineering where it is now routine to rely on CPU-intensive data analytics. Paradoxically, at the same time as needing more CPU, it now becomes harder to build faster ones (or power existing ones). To address this problem, much current research has explored <em>software autotuners</em>---tools for adjusting software such that they make better use of existing computational resources. For example, when autotuning a MapReduce algorithm, such tuners might decide how many nodes should be allocated to &quot;map&quot; and how many to &quot;reduce&quot;.</p>
<p>The space of possible options for autotuning is very large: all possible settings for algorithms, compilers, and execution time options. To complicate matters, there are many competing goals that could be used to guide that tuning; e.g. reducing usage of CPU _ and_ memory <em>and</em> power while ensuring good output from our algorithms. Hence, current research explores very <em>narrow autotuners</em> that explore just a few options while trying to improve on just one or two goals.</p>
<p>This research proposes very <em>broad autotuning</em> that takes advantage of the <em>synergies</em> between all those options and goals by exploiting <em>relevancy filtering</em> (to quickly dispose of unhelpful options), <em>locality of inference</em> (that enables faster updates to out-dated tunings) and <em>redundancy reduction</em> (that reduces the search space for better tunings).</p>
<hr />
<h2 id="tuning-data-miners">Tuning Data Miners</h2>
<p><em>2014 -- </em></p>
<p>All data miners are biased by the internal choices made during their search for models. Historically, learning the best tunings for a particular combination of <em>learner,dataSet</em> has been something of a black art. Here, we clarify and automate that process by applying simple optimizers (differential evolution). Findings so far include:</p>
<ul>
<li>Different dataSets/learners need different tunings. Hence we say:
<ul>
<li>It is no longer enough to apply such learners <em>of the shelf</em> using their default parameter settings.</li>
</ul></li>
<li>Tuning dramatically changes and improves the performance of a learners. Hence, we say that:
<ul>
<li>It is no longer sufficient to publish a data mining result <em>without</em> including a tuning pre-processor.</li>
</ul></li>
<li>It takes 100s to 1000s of trials to find good tunings. Running some learners (e.g. NaiveBayes) 1000 times is not problematic but other learners (e.g. Random Forests) can take days to tune. Hemce we say:
<ul>
<li>When selecting learners for a study, the runtime of that learner has now become a primary consideration.</li>
</ul></li>
<li>(The following comment is preliminary.) In terms of the moldes generated, tuning can radically change the learned model. Hence we say:
<ul>
<li>It is time to doubt, and revisit, prior truisms that certain infleuences of defects are dominate, or most informative, (e.g. Process or OO metrics are better than procedural metrics).</li>
</ul></li>
</ul>
<hr />
<h2 id="truisms-in-se-true-or-false">Truisms in SE: True or False?</h2>
<p><em>2014 -- </em></p>
<p>Following on from the last point, we are now collecting information on what hypotheses are most believed in the SE research and practitioner community, and checking if indeed they hold true.</p>
<p>Spoiler alert: no.</p>
<p>For example, recently we explored data from 100s of projects looking for the legendary <em>phase delay</em> effect (software fixes are exponentially more expensive to fix, the later you leave that fix). In short, no evidence was found for phase delay.</p>
<hr />
<h2 id="transfer-learning-in-software-engineering">Transfer Learning in Software Engineering</h2>
<p><em>2013 -- 2017</em></p>
<p>NSF, SHF: Medium: Collaborative, #1302216</p>
<p><img id=pad src="img/nsf1v.jpg"></p>
<p>The goal of the research is to enable software engineers to find software development best practices from past empirical data. The increasing availability of software development project data, plus new machine learning techniques, make it possible for researchers to study the generalizability of results across projects using the concept of transfer learning. Using data from real software projects, the project will determine and validate best practices in three areas: predicting software development effort; isolating software detects; effective code inspection practices.</p>
<p>This research will deliver new data mining technologies in the form of transfer learning techniques and tools that overcome current limitations in the state-of-the-art to provide accurate learning within and across projects. It will design new empirical studies, which apply transfer learning to empirical data collected from industrial software projects. It will build an on-line model analysis service, making the techniques and tools available to other researchers who are investigating validity of principles for best practice.</p>
<p>The broader impacts of the research will be to make empirical software engineering research results more transferable to practice, and to improve the research processes for the empirical software engineering community. By providing a means to test principles about software development, this work stands to transform empirical software engineering research and enable software managers to rely on scientifically obtained facts and conclusions rather than anecdotal evidence and one-off studies. Given the immense importance and cost of software in commercial and critical systems, the research has long-term economic impacts.</p>
<hr />
<h2 id="gale-geometric-active-learning-for-search-based-software-engineering">GALE: Geometric Active Learning for Search-Based Software Engineering</h2>
<p>with <em>Joseph Krall,</em>, WVU</p>
<p><img id=pad width=300 src="img/gale.png"> Multi-objective evolutionary algorithms (MOEAs) help software engineers find novel solutions to complex problems. When MOEAs explore too many options, they are slow to use and hard to comprehend. GALE is a near-linear time MOEA that builds a piecewise approximation to the surface of best solutions along the Pareto frontier. For each piece, GALE mutates solutions towards the better end. In numerous case studies, GALE finds comparable solutions to standard methods (NSGA-II, SPEA2) using far fewer evaluations (e.g. 20 evaluations, not 1000). GALE is recommended when a model is expensive to evaluate, or when some audience needs to browse and understand how an MOEA has made its conclusions.</p>
<p><br clear=all></p>
<hr />
<h2 id="evolutionary-search-with-strong-heuristics-for-software-product-line-configuration">Evolutionary Search with Strong Heuristics for Software Product Line Configuration</h2>
<p>with <em>Abdel Salam Sayyad,</em> WVU</p>
<p><img id=pad width=300 src="img/spl.png"> Software design is a process of trading off competing objectives. In this study, we configure software product lines (expressed as feature models). As we increase the number of objectives, standard optimizers in widespread use (e.g. NSGA-II, SPEA2) perform much worse than IBEA (Indicator-Based Evolutionary Algorithm) since IBEA makes most use of user preferences. Also, IBEA generates far more products with no violations of domain constraints. This research presents two methods for scaling IBEA to very large feature models with many objectives. Our “PUSH” technique forces the evolutionary search to respect certain rules and dependencies defined by the feature models. Also, our “PULL” technique gives higher weight to constraint satisfaction as an optimization objective and thus achieves a higher percentage of fully-compliant configurations within short runtimes. Using IBEA+PUSH+PULL, we can extract valid products in a matter of minutes, even from very large feature models of Linux kernels. Our conclusion is that the methods we apply in search-based software engineering need to be carefully chosen, particularly when studying complex decision spaces with many optimization objectives. As shown here, better and faster optimizers can be built when designers take full advantage of naturally occurring domain constraints.</p>
<p><br clear=all></p>
<hr />
<h2 id="lace2-better-privacy-preserving-data-sharing-for-cross-project-defect-prediction">LACE2: Better Privacy-Preserving Data Sharing for Cross Project Defect Prediction</h2>
<p>with <em>Fayola Peters</em>, Lero, Irish SE Research Centre</p>
<p><img id=pad width=200 src="img/lace.png"> Before a community can learn general principles, it must share individual experiences. A wide range of privacy con- siderations complicates sharing of data in software engineering. Prior work on secure data sharing allowed data owners to share their data on a single-party basis.</p>
<p>LACE2 extends that work by considering multi-party data sharing where data owners incrementally add data to a cache passed between them. Only a portion of local data is added to this cache: the “interesting” data that are not similar to the current contents of the cache. Also, before data owner i passes the cache to data owner j, privacy is preserved by applying obfuscation algorithms to hide project details.</p>
<p>The experiments of this research show that (a) LACE2 is comparatively less expensive than the single-party approach and (b) the multi-party approach of LACE2 yields higher privacy than the prior approach without damaging predictive power (indeed, in some cases, LACE2 lead to better defect predictors).</p>
<p><br clear=all></p>
<hr />
<h2 id="years-of-parametric-effort-estimation-a-report-card-on-cocomo-style-research">40 Years of Parametric Effort Estimation: A Report Card on COCOMO-style Research</h2>
<p>with <em>Barry Boehm, Ye Yang, Jairus Hihn,</em> from USC, Stevens Institute, JPL</p>
<p><img id=pad  width=300 src="img/cocreport.png"> The longevity of parametric effort estimation is remarkable. Decades after their invention, these methods are still both widely used and widely useful.</p>
<p>This research reviews the standard criticisms of this approach. We find that, contrary to common criticisms, (1) parametric estimation has not been superseded by more recent estimation methods; (2) it is not true that parametric estimation is no better than simplistic lines of code counts; (3) the old parametric calibration data is still relevant to more recent projects; (4) parametric estimation need not be expensive to deploy at some new site since these these methods can be tuned on very small sample sizes (in our experiments, a mere eight projects is enough); and (5) compared to other methods, parametric estimation is not unduly sensitive to errors in the size estimates.</p>
<p>Hence we conclude that, in 2015, is still valid and recommended practice to try parametric estimation before exploring other, more innovative methods. Also, it can be useful to augment parametric estimation with (a) some local calibration and (b) some column pruning (examples of those techniques are discussed in this research).</p>
<p><br clear=all></p>
<hr />
<h2 id="cross-trees-visualizing-estimations-using-decision-trees">Cross Trees: Visualizing Estimations using Decision Trees</h2>
<p>with <em>Naveen Kumar Lekkalapudi,</em> WVU</p>
<p><img id=pad width=300 src="img/crosstrees.png"> Optimization has been the goal of almost every human thought and action. With growing computational capabilities, solutions to problems are also exponentially increasing. Literature proves that with rising demand for data and analytics on this large data, solutions to problems are multiplied. These solutions are supported with strong statistical and analytical reasoning that does help experts narrow down solutions. Optimizing these large set of solutions can get tricky to experts who seek knowledge of how these solutions have improved and what decisions need to be taken for remaining solutions to improve. This problem persists in software engineering with managers and experts taking decisions which decide the course of project.</p>
<p>This thesis proposes a method for optimizing solutions along with providing decisions that help improve a solution. Literature supports that landscape visualization of data gives an inside scoop of data behaviour. A method is proposed which takes benefit of visualizing data and improving solutions based on their position in landscape. Cross Trees are built by grouping data based on their similarities. Traversing from bad group of solutions to better group of solutions require few decisions to taken. These decisions are proposed by CrossTree and are tested for how valid they are. CrossTree is tested with two models which simulate software projects-POM3 and XOMO. Also, models are simluated with one of the best genetic algortihms NSGA-II, to generate set of optimized solutions. CrossTree is compared against results from NSGA-II to validate performance.</p>
    </div>

    <div id="footer">
      <p><b>&copy; 2015 Tim Menzies.</b></p>
    </div>
  </div>
</body>
</html>
